{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sprint 自然言語処理入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDBをカレントフォルダにダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\r\n",
      "\r\n",
      "Overview\r\n",
      "\r\n",
      "This dataset contains movie reviews along with their associated binary\r\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\r\n",
      "sentiment classification. This document outlines how the dataset was\r\n",
      "gathered, and how to use the files provided. \r\n",
      "\r\n",
      "Dataset \r\n",
      "\r\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\r\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\r\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\r\n",
      "documents for unsupervised learning. \r\n",
      "\r\n",
      "In the entire collection, no more than 30 reviews are allowed for any\r\n",
      "given movie because reviews for the same movie tend to have correlated\r\n",
      "ratings. Further, the train and test sets contain a disjoint set of\r\n",
      "movies, so no significant performance is obtained by memorizing\r\n",
      "movie-unique terms and their associated with observed labels.  In the\r\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\r\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\r\n",
      "more neutral ratings are not included in the train/test sets. In the\r\n",
      "unsupervised set, reviews of any rating are included and there are an\r\n",
      "even number of reviews > 5 and <= 5.\r\n",
      "\r\n",
      "Files\r\n",
      "\r\n",
      "There are two top-level directories [train/, test/] corresponding to\r\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\r\n",
      "the reviews with binary labels positive and negative. Within these\r\n",
      "directories, reviews are stored in text files named following the\r\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\r\n",
      "the star rating for that review on a 1-10 scale. For example, the file\r\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\r\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\r\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\r\n",
      "omitted for this portion of the dataset.\r\n",
      "\r\n",
      "We also include the IMDb URLs for each review in a separate\r\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\r\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\r\n",
      "are unable to link directly to the review, but only to the movie's\r\n",
      "review page.\r\n",
      "\r\n",
      "In addition to the review text files, we include already-tokenized bag\r\n",
      "of words (BoW) features that were used in our experiments. These \r\n",
      "are stored in .feat files in the train/test directories. Each .feat\r\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\r\n",
      "data.  The feature indices in these files start from 0, and the text\r\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\r\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\r\n",
      "(the) appears 7 times in that review.\r\n",
      "\r\n",
      "LIBSVM page for details on .feat file format:\r\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\r\n",
      "\r\n",
      "We also include [imdbEr.txt] which contains the expected rating for\r\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\r\n",
      "rating is a good way to get a sense for the average polarity of a word\r\n",
      "in the dataset.\r\n",
      "\r\n",
      "Citing the dataset\r\n",
      "\r\n",
      "When using this dataset please cite our ACL 2011 paper which\r\n",
      "introduces it. This paper also contains classification results which\r\n",
      "you may want to compare against.\r\n",
      "\r\n",
      "\r\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\r\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\r\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\r\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\r\n",
      "  month     = {June},\r\n",
      "  year      = {2011},\r\n",
      "  address   = {Portland, Oregon, USA},\r\n",
      "  publisher = {Association for Computational Linguistics},\r\n",
      "  pages     = {142--150},\r\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\r\n",
      "}\r\n",
      "\r\n",
      "References\r\n",
      "\r\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\r\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\r\n",
      "636-659.\r\n",
      "\r\n",
      "Contact\r\n",
      "\r\n",
      "For questions/comments/corrections please contact Andrew Maas\r\n",
      "amaas@cs.stanford.edu\r\n"
     ]
    }
   ],
   "source": [
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz\n",
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup\n",
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "print(\"x : {}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.古典的な手法\n",
    "古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。\n",
    "\n",
    "## 5.BoW\n",
    "単純ながら効果的な方法として BoW (Bag of Words) があります。これは、サンプルごとに単語などの 登場回数 を数えたものをベクトルとする方法です。単語をカテゴリとして捉え one-hot表現 していることになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "# DataFrameにまとめる\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ 語彙 と呼びます。\n",
    "\n",
    "\n",
    "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを コーパス と呼びます。語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "CountVectorizerクラスでは大文字は小文字に揃えるという 前処理 が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（テキストクリーニング）や表記揺れの統一といったことを別途行うことが一般的です。\n",
    "\n",
    "\n",
    "語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える ステミング と呼ばれる処理を行うこともあります。\n",
    "\n",
    "## トークン\n",
    "BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。\n",
    "\n",
    "\n",
    "何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n",
    "\n",
    "\n",
    "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。\n",
    "\n",
    "\n",
    "《正規表現》\n",
    "\n",
    "\n",
    "正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
    "\n",
    "\n",
    "re — 正規表現操作\n",
    "https://docs.python.org/ja/3/library/re.html\n",
    "\n",
    "\n",
    "正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
    "\n",
    "\n",
    "Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript\n",
    "https://regex101.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析\n",
    "英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
    "\n",
    "\n",
    "日本語では名詞や助詞、動詞のように異なる 品詞 で分けられる単位で 分かち書き することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
    "\n",
    "\n",
    "これには MeCab や Janome のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb茶まめというサービスも国立国語研究所が提供しています。\n",
    "\n",
    "\n",
    "自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として mecab-ipadic-NEologd がオープンソースで存在しています\n",
    "https://github.com/neologd/mecab-ipadic-neologd/blob/master/README.ja.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram\n",
    "上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順は全く考慮されていません。\n",
    "\n",
    "\n",
    "考慮するために、隣あう単語同士をまとめて扱う n-gram という考え方を適用することがあります。2つの単語をまとめる場合は 2-gram (bigram) と呼び、次のようになります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  is very  movie is  this film  this movie  \\\n",
       "0       0         0        0     0        1         1          0           1   \n",
       "1       1         0        1     1        0         0          1           0   \n",
       "2       0         1        0     0        0         0          0           0   \n",
       "\n",
       "   very bad  very good  very very  \n",
       "0         0          1          0  \n",
       "1         0          0          0  \n",
       "2         2          0          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ngram_rangeで利用するn-gramの範囲を指定する\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-gramにより「very good」と「very bad」が区別して数えられています。\n",
    "\n",
    "\n",
    "単語をまとめない場合は 1-gram (unigram) と呼びます。3つまとめる3-gram(trigram)など任意の数を考えることができます。1-gramと2-gramを組み合わせてBoWを行うといったこともあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】BoWのスクラッチ実装\n",
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    "- This movie is SOOOO funny!!!\n",
    "- What a movie! I never\n",
    "- best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScratchBoW(text, gram):\n",
    "    word_list = []\n",
    "    if gram == 1:\n",
    "        for i in range(len(text)):\n",
    "            token = re.split('[, /]',text[i])\n",
    "            #token = re.split(r'(?u)\\b\\w+\\b',text[i])\n",
    "            for j in range(len(token)):\n",
    "                word_list.append(token[j])\n",
    "    else:\n",
    "        for i in range(len(text)):\n",
    "            token = re.split('[, /]',text[i])\n",
    "            for j in range(len(token) - 1):\n",
    "                word_list.append(token[j] + \" \" + token[j + 1])\n",
    "                \n",
    "    word_label = list(set(word_list))\n",
    "    mat = np.zeros([len(text), len(word_label)])\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        for j in range(len(word_label)):\n",
    "            mat[i][j] = text[i].count(word_label[j])\n",
    "    print(word_label)\n",
    "    print(mat)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This movie is SOOOO funny!!!\",\n",
    "\"What a movie! I never\",\n",
    "\"best movie ever!!!!! this movie\"]\n",
    "for i in range(len(text)):\n",
    "    text[i] = text[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'what', 'movie!', 'movie', 'best', 'ever!!!!!', 'i', 'funny!!!', 'this', 'is', 'never', 'soooo']\n",
      "[[0. 0. 0. 1. 0. 0. 3. 1. 1. 2. 0. 1.]\n",
      " [2. 1. 1. 1. 0. 0. 2. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 2. 1. 1. 3. 0. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ScratchBoW(text, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie ever!!!!!', 'ever!!!!! this', 'soooo funny!!!', 'is soooo', 'movie is', 'a movie!', 'what a', 'movie! i', 'best movie', 'this movie', 'i never']\n",
      "[[0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ScratchBoW(text, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.TF-IDF\n",
    "BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。\n",
    "\n",
    "\n",
    "《標準的なTF-IDFの式》\n",
    "\n",
    "Term Frequency:\n",
    "$$\n",
    "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}\n",
    "$$\n",
    "\n",
    " $n_{t,d}$ : サンプルd内のトークンtの出現回数（BoWと同じ）\n",
    "\n",
    "\n",
    "$\\sum_{s \\in d}n_{s,d}$ : サンプルdの全トークンの出現回数の和\n",
    "\n",
    "\n",
    "Inverse Document Frequency:\n",
    "$$\n",
    "idf(t) = \\log{\\frac{N}{df(t)}}\n",
    "$$\n",
    "N : サンプル数\n",
    "\n",
    " $df(t)$: トークンtが出現するサンプル数\n",
    "\n",
    "\n",
    "＊logの底は任意の値\n",
    "\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "$$\n",
    "tfidf(t, d) = tf(t, d) \\times idf(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF\n",
    "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
    "\n",
    "\n",
    "サンプル数 NをIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$\n",
    " を変化させたグラフを確認してみると、次のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n_samples = 25000\n",
    "idf = np.log(n_samples/np.arange(1,n_samples))\n",
    "plt.title(\"IDF\")\n",
    "plt.xlabel(\"df(t)\")\n",
    "plt.ylabel(\"IDF\")\n",
    "plt.plot(idf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ストップワード\n",
    "あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを ストップワード と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  movie  this  very\n",
       "0  0    0     0     1      1     1     1\n",
       "1  1    0     1     1      0     1     0\n",
       "2  0    2     0     0      0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/arisa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆に、登場回数が特に少ないトークンも取り除くことが多いです。全てのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
    "\n",
    "\n",
    "scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  good  is  this  very\n",
       "0    0     1   1     1     1\n",
       "1    0     1   1     1     0\n",
       "2    2     0   0     0     3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】TF-IDFの計算\n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
    "\n",
    "\n",
    "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "\n",
    "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
    "\n",
    "\n",
    "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n",
    "\n",
    "\n",
    "Term Frequency:\n",
    "$$\n",
    "tf(t,d) = n_{t,d}\n",
    "$$\n",
    "\n",
    "$n_{t,d}$: サンプルd内のトークンtの出現回数\n",
    "scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n",
    "\n",
    "\n",
    "Inverse Document Frequency:\n",
    "$$\n",
    "idf(t) = \\log{\\frac{1+N}{1+df(t)}}+1\n",
    "$$\n",
    "\n",
    "N : サンプル数\n",
    "\n",
    "\n",
    "$df(t)$: トークンtが出現するサンプル数\n",
    "\n",
    "\n",
    "＊logの底はネイピア数e\n",
    "\n",
    "\n",
    "詳細は以下のドキュメントを確認してください。\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題2の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_vec = vectorizer.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 74849)\n"
     ]
    }
   ],
   "source": [
    "print(X_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】TF-IDFを用いた学習\n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_vec, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "yp_base = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率： 0.8838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"正解率：\",accuracy_score(yp_base, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワード使用、最大語彙数50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大語彙数を50にして軽いデータを作成\n",
    "vectorizer_50= TfidfVectorizer(stop_words=stop_words, max_features=50, norm=None)\n",
    "x_vec_50 = vectorizer_50.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(x_vec_50, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_50 = LogisticRegression()\n",
    "clf_50.fit(X_train, Y_train)\n",
    "yp_50 = clf_50.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率： 0.7096\n"
     ]
    }
   ],
   "source": [
    "print(\"正解率：\",accuracy_score(yp_50, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワードなし、最大語彙数50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大語彙数を50にして軽いデータを作成\n",
    "vectorizer_non50= TfidfVectorizer(max_features=50, norm=None)\n",
    "x_vec_non50 = vectorizer_non50.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(x_vec_non50, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_non50 = LogisticRegression()\n",
    "clf_non50.fit(X_train, Y_train)\n",
    "yp_non50 = clf_non50.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率： 0.6534\n"
     ]
    }
   ],
   "source": [
    "print(\"正解率：\",accuracy_score(yp_non50, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ストップワードあり、2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大語彙数を50にして軽いデータを作成\n",
    "vectorizer_2gram= TfidfVectorizer(stop_words=stop_words, ngram_range=(2, 2), max_features=50, norm=None)\n",
    "x_vec_2gram = vectorizer_2gram.fit_transform(x_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(x_vec_2gram, y_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_2gram = LogisticRegression()\n",
    "clf_2gram.fit(X_train, Y_train)\n",
    "yp_2gram = clf_2gram.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率： 0.6166\n"
     ]
    }
   ],
   "source": [
    "print(\"正解率：\",accuracy_score(yp_2gram, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考察：ストップワード有りの方が精度が高かった。gram1の方が精度が高かった。最大語彙数を減らすと正解率も大きく下がったが、処理の時間も短くなった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装\n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。<br>\n",
    "標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。<br>\n",
    "\n",
    "- This movie is SOOOO funny!!!\n",
    "- What a movie! I never\n",
    "- best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "'This movie is SOOOO funny!!!',\n",
    "'What a movie! I never',\n",
    "'best movie ever!!!!! this movie'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "text_vec = vectorizer.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'ever', 'funny', 'is', 'movie', 'never', 'soooo', 'this', 'what']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>soooo</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       best      ever     funny        is  movie     never     soooo  \\\n",
       "0  0.000000  0.000000  1.693147  1.693147    1.0  0.000000  1.693147   \n",
       "1  0.000000  0.000000  0.000000  0.000000    1.0  1.693147  0.000000   \n",
       "2  1.693147  1.693147  0.000000  0.000000    2.0  0.000000  0.000000   \n",
       "\n",
       "       this      what  \n",
       "0  1.287682  0.000000  \n",
       "1  0.000000  1.693147  \n",
       "2  1.287682  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_train = (vectorizer.fit_transform(text).toarray())\n",
    "df = pd.DataFrame(tf_idf_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチ：標準的な式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScratchTFIDF(text):\n",
    "    word_list = []\n",
    "    for i in range(len(text)):\n",
    "        token = re.split('[, /]',text[i])\n",
    "        for j in range(len(token)):\n",
    "            word_list.append(token[j])\n",
    "                \n",
    "    word_label = list(set(word_list))\n",
    "    mat = np.zeros([len(text), len(word_label)])\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        for j in range(len(word_label)):\n",
    "            tf = text[i].count(word_label[j]) / len(re.split('[, /]',text[i]))\n",
    "            count = 0\n",
    "            for k in range(len(text)):\n",
    "                if 0 < text[k].count(word_label[j]):\n",
    "                    count += 1\n",
    "            idf = np.log(len(text)/count)\n",
    "            mat[i][j] = tf*idf\n",
    "    #print(word_label)\n",
    "    #print(mat)\n",
    "    df = pd.DataFrame(mat, columns=word_label)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>I</th>\n",
       "      <th>movie!</th>\n",
       "      <th>movie</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>best</th>\n",
       "      <th>ever!!!!!</th>\n",
       "      <th>never</th>\n",
       "      <th>funny!!!</th>\n",
       "      <th>What</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>This</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>0.219722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.439445</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         I    movie!  movie     SOOOO      best  ever!!!!!  \\\n",
       "0  0.000000  0.000000  0.000000    0.0  0.219722  0.000000   0.000000   \n",
       "1  0.439445  0.219722  0.219722    0.0  0.000000  0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.000000    0.0  0.000000  0.219722   0.219722   \n",
       "\n",
       "      never  funny!!!      What      this        is      This  \n",
       "0  0.000000  0.219722  0.000000  0.000000  0.162186  0.219722  \n",
       "1  0.219722  0.000000  0.219722  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.219722  0.081093  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ScratchTFIDF(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクラッチ：scikit-learnの採用している式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScratchTFIDF_SK(text):\n",
    "    word_list = []\n",
    "    for i in range(len(text)):\n",
    "        token = re.split('[, /]',text[i])\n",
    "        for j in range(len(token)):\n",
    "            word_list.append(token[j])\n",
    "                \n",
    "    word_label = list(set(word_list))\n",
    "    mat = np.zeros([len(text), len(word_label)])\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        for j in range(len(word_label)):\n",
    "            tf = text[i].count(word_label[j])\n",
    "            count = 0\n",
    "            for k in range(len(text)):\n",
    "                if 0 < text[k].count(word_label[j]):\n",
    "                    count += 1\n",
    "            idf = np.log((len(text) + 1)/(count + 1)) + 1\n",
    "            mat[i][j] = tf*idf\n",
    "    #print(word_label)\n",
    "    #print(mat)\n",
    "    df = pd.DataFrame(mat, columns=word_label)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>I</th>\n",
       "      <th>movie!</th>\n",
       "      <th>movie</th>\n",
       "      <th>SOOOO</th>\n",
       "      <th>best</th>\n",
       "      <th>ever!!!!!</th>\n",
       "      <th>never</th>\n",
       "      <th>funny!!!</th>\n",
       "      <th>What</th>\n",
       "      <th>this</th>\n",
       "      <th>is</th>\n",
       "      <th>This</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.575364</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.386294</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          a         I    movie!  movie     SOOOO      best  ever!!!!!  \\\n",
       "0  0.000000  0.000000  0.000000    1.0  1.693147  0.000000   0.000000   \n",
       "1  3.386294  1.693147  1.693147    1.0  0.000000  0.000000   0.000000   \n",
       "2  0.000000  0.000000  0.000000    2.0  0.000000  1.693147   1.693147   \n",
       "\n",
       "      never  funny!!!      What      this        is      This  \n",
       "0  0.000000  1.693147  0.000000  0.000000  2.575364  1.693147  \n",
       "1  1.693147  0.000000  1.693147  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  1.693147  1.287682  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ScratchTFIDF_SK(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_sc= TfidfVectorizer(norm=None)\n",
    "x_vec_sc = vectorizer_sc.fit_transform(text).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>soooo</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.287682</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       best      ever     funny        is  movie     never     soooo  \\\n",
       "0  0.000000  0.000000  1.693147  1.693147    1.0  0.000000  1.693147   \n",
       "1  0.000000  0.000000  0.000000  0.000000    1.0  1.693147  0.000000   \n",
       "2  1.693147  1.693147  0.000000  0.000000    2.0  0.000000  0.000000   \n",
       "\n",
       "       this      what  \n",
       "0  1.287682  0.000000  \n",
       "1  0.000000  1.693147  \n",
       "2  1.287682  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(x_vec_sc, columns=vectorizer_sc.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "ニューラルネットワークを用いてベクトル化を行う手法が Word2Vec です。\n",
    "\n",
    "\n",
    "BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを Word Embedding（単語埋め込み） や 分散表現 と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n",
    "\n",
    "\n",
    "Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBoW\n",
    "CBoW (Continuous Bag-of-Words) によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n",
    "\n",
    "\n",
    "単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n",
    "\n",
    "\n",
    "間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n",
    "\n",
    "\n",
    "あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える パディング を行なったり、長いテキストは単語を消したりします。テキストを 固定長 にすると呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ウィンドウサイズ\n",
    "入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを ウィンドウサイズ と呼びます。\n",
    "\n",
    "### Skip-gram\n",
    "CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が Skip-gram です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。\n",
    "\n",
    "### 利用方法\n",
    "Pythonでは Gensim ライブラリを用いて扱うことができます。\n",
    "https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n",
    "\n",
    "\n",
    "デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[-0.02662116  0.0176387  -0.04640723 -0.03603607 -0.03983193  0.04744475\n",
      "  0.02080979  0.03100929 -0.03336754  0.02293834]\n",
      "movieのベクトル : \n",
      "[ 0.00374345 -0.04126974  0.00712079 -0.01543243  0.04141569 -0.03266859\n",
      " -0.03722949  0.04804384  0.00840449 -0.0373885 ]\n",
      "isのベクトル : \n",
      "[ 4.1133519e-03  3.0619601e-02  4.6578422e-02  3.5008822e-02\n",
      " -2.1143414e-02  4.5892783e-02  1.0605002e-05  5.0248657e-03\n",
      "  3.3616841e-02 -2.3979645e-02]\n",
      "veryのベクトル : \n",
      "[-0.02023077 -0.03912916  0.01064228 -0.04079828 -0.0392892  -0.04185558\n",
      "  0.03741433 -0.03298487 -0.00768175 -0.00613079]\n",
      "goodのベクトル : \n",
      "[-0.03700087 -0.03591486  0.01200262 -0.04835247  0.04490133 -0.00122542\n",
      "  0.02655091 -0.02727574 -0.0154673  -0.00153656]\n",
      "filmのベクトル : \n",
      "[ 0.0370249  -0.00968983 -0.02703274  0.01699764  0.00015523  0.04652674\n",
      "  0.00407525 -0.0199183  -0.04388898  0.01611501]\n",
      "aのベクトル : \n",
      "[-0.00787545  0.00202103  0.02759698  0.03952957 -0.03291098 -0.00627121\n",
      "  0.00222605 -0.01439413  0.00491561  0.0014832 ]\n",
      "badのベクトル : \n",
      "[-0.04636762 -0.04109469 -0.0370338  -0.01659404 -0.01007185 -0.04287139\n",
      "  0.03201649  0.03327038 -0.022942    0.02829041]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の距離\n",
    "ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('very', 0.49509215354919434),\n",
       " ('bad', 0.3468261659145355),\n",
       " ('movie', 0.1815365105867386)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化\n",
    "2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB映画レビューデータセットの分散表現\n",
    "IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】コーパスの前処理\n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルデータで確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "'This movie is SOOOO funny!!!',\n",
    "'What a movie! I never',\n",
    "'best movie ever!!!!! this movie'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for i in range(len(text)):\n",
    "    text[i] = text[i].lower()\n",
    "    #text[i] = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text[i])\n",
    "    text[i] = text[i].replace(r\"[^a-zA-Z0-9]+\", \"\")\n",
    "    text[i] = text[i].rstrip()\n",
    "    token = re.split('[, /]',text[i])\n",
    "    text_list.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'movie', 'is', 'soooo', 'funny'],\n",
       " ['what', 'a', 'movie', 'i', 'never'],\n",
       " ['best', 'movie', 'ever', 'this', 'movie']]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'soooo', 'funny', 'what', 'a', 'i', 'never', 'best', 'ever'])\n",
      "thisのベクトル : \n",
      "[-0.02662116  0.0176387  -0.04640723 -0.03603607 -0.03983193  0.04744475\n",
      "  0.02080979  0.03100929 -0.03336754  0.02293834]\n",
      "movieのベクトル : \n",
      "[ 0.00374345 -0.04126974  0.00712079 -0.01543243  0.04141569 -0.03266859\n",
      " -0.03722949  0.04804384  0.00840449 -0.0373885 ]\n",
      "isのベクトル : \n",
      "[ 4.1133263e-03  3.0619409e-02  4.6578132e-02  3.5008602e-02\n",
      " -2.1143282e-02  4.5892496e-02  1.0604936e-05  5.0248341e-03\n",
      "  3.3616632e-02 -2.3979494e-02]\n",
      "sooooのベクトル : \n",
      "[ 0.03220742  0.00519683  0.03903654  0.0149874   0.03208504  0.02011322\n",
      "  0.02522402 -0.02201126  0.04153448 -0.02762788]\n",
      "funnyのベクトル : \n",
      "[ 0.04080604  0.0122753   0.01304101  0.00276528  0.01816266 -0.0439434\n",
      "  0.01466237 -0.04031313 -0.02920861 -0.04165947]\n",
      "whatのベクトル : \n",
      "[-0.02561819 -0.02223832 -0.00350261  0.03779     0.03028934  0.02773167\n",
      "  0.00778862 -0.04865639  0.01095658 -0.00097947]\n",
      "aのベクトル : \n",
      "[-0.00787545  0.00202103  0.02759698  0.03952957 -0.03291098 -0.00627121\n",
      "  0.00222605 -0.01439413  0.00491561  0.0014832 ]\n",
      "iのベクトル : \n",
      "[-0.03599725  0.0452351  -0.03276157 -0.01798938  0.02654491 -0.01725884\n",
      "  0.01568943  0.03196847  0.02081504  0.04291203]\n",
      "neverのベクトル : \n",
      "[-0.00735685 -0.03170357  0.00084777 -0.04576418 -0.00812105 -0.03537975\n",
      "  0.01482093  0.00965146 -0.04460959  0.03664199]\n",
      "bestのベクトル : \n",
      "[ 0.0396817   0.00911481 -0.03790594  0.03934394  0.02177743 -0.02998122\n",
      " -0.02136018 -0.03783012 -0.02451865  0.03766344]\n",
      "everのベクトル : \n",
      "[ 0.02645831  0.0149371  -0.00118399 -0.02895552 -0.04974873 -0.032411\n",
      " -0.00391655 -0.01105063 -0.01997422  0.03196385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(text_list) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabs = model.wv.vocab.keys()\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
    "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
    "for i, word in enumerate(list(vocabs)):\n",
    "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "X_test = x_test.copy()\n",
    "for i in range(len(x_test)):\n",
    "    X_test[i] = X_test[i].lower()\n",
    "    #text[i] = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text[i])\n",
    "    X_test[i] = X_test[i].replace(r\"[^a-zA-Z0-9]+\", \"\")\n",
    "    X_test[i] = X_test[i].rstrip()\n",
    "    token = re.split('[, /]',X_test[i])\n",
    "    text_list.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題6】Word2Vecの学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56, 75)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(text_list) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('role;', 0.9398078322410583),\n",
       " ('seppuku!', 0.9374410510063171),\n",
       " ('post-war.<br', 0.9360769391059875),\n",
       " ('cancerman)', 0.9313536882400513),\n",
       " ('cos', 0.9304590821266174),\n",
       " (\"lucille's\", 0.9262431859970093),\n",
       " ('hands...<br', 0.9242637753486633),\n",
       " ('01.', 0.9240878224372864),\n",
       " ('schooler', 0.9189344048500061),\n",
       " ('pasted', 0.91874760389328)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"good\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('down-vote', 0.9682837724685669),\n",
       " ('15%(the', 0.9645987749099731),\n",
       " ('tamakichi', 0.9593921303749084),\n",
       " ('odnt', 0.9520321488380432),\n",
       " ('arau)', 0.9451302289962769),\n",
       " ('ross(ala', 0.9448950290679932),\n",
       " ('dio...', 0.9437155723571777),\n",
       " ('honourable', 0.9341142773628235),\n",
       " ('!)..', 0.9306801557540894),\n",
       " ('shirt\"', 0.9290185570716858)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=\"bad\", topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "247.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
