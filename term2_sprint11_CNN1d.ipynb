{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深層学習スクラッチ　畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装：問題1〜3\n",
    "- 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "- 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "- 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d_q1_q3:\n",
    "    def __init__(self, lr=0.01, sigma=0.01,n_features=784, n_filters=3, n_output=10):\n",
    "        \n",
    "        self.lr = lr     # 学習係数\n",
    "        self.sigma = sigma    # 重みとバイアスの広がり具合\n",
    "        self.n_features = n_features\n",
    "        self.n_filters = n_filters    #フィルターのサイズ\n",
    "        self.n_output = n_output\n",
    "        \n",
    "    def fit(self, X, w, b, stride=1,pad=0):\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.X = X\n",
    "        n_nodes1 = X.shape[0]\n",
    "        # 出力サイズを計算\n",
    "        n_nodes2 = self.calc_out_shape(n_nodes1)\n",
    "        A1 = np.zeros(n_nodes2)\n",
    "        A1 = self.forward(A1, n_nodes2, w, b)\n",
    "        print(\"A1\", A1)\n",
    "        delta_a = np.array([10, 20])\n",
    "        db = sum(delta_a)\n",
    "        print(\"db\", db)\n",
    "        dw = np.zeros(w.shape[0])\n",
    "        dw = self.backward(delta_a, n_nodes2, dw)\n",
    "        print(\"dw\", dw)\n",
    "        \n",
    "        dx = np.zeros(self.X.shape[0])\n",
    "\n",
    "        for i in range(self.X.shape[0]):\n",
    "            s = 0\n",
    "            for j in range(n_nodes2):\n",
    "                if not ((i - j) < 0 or (i - j) > n_nodes2):\n",
    "                    s += delta_a[j] * w[i - j]\n",
    "            dx[i] = s\n",
    "        print(\"dx\", dx)\n",
    "            \n",
    "        \n",
    "    def forward(self, A1, n_nodes2, w, b):\n",
    "        for i in range(n_nodes2):\n",
    "            A1[i] = sum(self.X[i : (i + self.n_filters)] * w) + b\n",
    "        return A1\n",
    "        \n",
    "    def backward(self, delta_a, n_nodes2, dw):\n",
    "        for i in range(dw.shape[0]):\n",
    "            dw[i] = np.dot(delta_a, self.X[i : (i + n_nodes2)])\n",
    "        return dw\n",
    "        \n",
    "            \n",
    "    def calc_out_shape(self, n_nodes1):\n",
    "        # 出力サイズの計算\n",
    "        n_nodes2 = 1 + (n_nodes1 + 2*self.pad - self.n_filters)/self.stride\n",
    "        return int(n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 [35. 50.]\n",
      "db 30\n",
      "dw [ 50.  80. 110.]\n",
      "dx [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "sc_conv1 = SimpleConv1d_q1_q3(n_features=4)\n",
    "sc_conv1.fit(x, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装：問題4、問題8\n",
    "- 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "- 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設計：バッチサイズ1\n",
    "\n",
    "Ich:入力チャンネル、n_features：入力特徴量数、Och:出力チャンネル、Fw:フィルタ幅、Ow:出力幅、n_output：最終出力幅、B:バッチサイズ\n",
    "\n",
    "【流れ】<br>\n",
    "X=(Ich, W)、w(フィルタ)=(Och,Ich,Fw)、B=(Och,)<br>\n",
    "→出力A=(Och,Ow)→活性化関数後Z=(Och,Ow)→フラットにするZ=(B,Och*Ow)→<br>\n",
    "FC層w=(Och*Ow, n_output)、B=(n_output,)→yp(B,n_output)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逆伝播FC→convのreshapeで戻す動きを念のため確認<br>\n",
    "reshape(Och, Ow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test = np.array([[1, 2],\n",
    "                  [3, 4],\n",
    "                  [5, 6]])\n",
    "A_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test = A_test.reshape(1, 6)\n",
    "A_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_test.reshape(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設計：バッチサイズ複数\n",
    "Ich:入力チャンネル、n_features：入力特徴量数、Och:出力チャンネル、Fw:フィルタ幅、Ow:出力幅、n_output：最終出力幅、B:バッチサイズ\n",
    "\n",
    "【流れ】<br>\n",
    "X=(B,Ich, W)、w(フィルタ)=(B,Och,Ich,Fw)、B=(B,Och)<br>\n",
    "→出力A=(B,Och,Ow)→活性化関数後Z=(B,Och,Ow)→フラットにするZ=(B,Och*Ow)→<br>\n",
    "FC層w=(Och*Ow, n_output)、B=(n_output,)→yp(B,n_output)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期値クラス SimpleInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2, Fil_w=None):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        FC層\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        \n",
    "        conv層(Out_ch、In_ch、Fil_w)\n",
    "        n_nodes1 : int\n",
    "          後の層のノード数\n",
    "        n_nodes2 : int\n",
    "          前の層のノード数\n",
    "        Fil_w  :  int\n",
    "           フィルタ幅\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        if Fil_w == None:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        else:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2, Fil_w)\n",
    "\n",
    "#         問4確認用\n",
    "#         W = np.array([[[1., 1., 2.], [2., 1., 1.]],\n",
    "#                       [[2., 1., 1.], [1., 1., 1.]],\n",
    "#                       [[1., 1., 1.], [1., 1., 1.]]])\n",
    "#         W = np.ones((3, 2, 3))\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "#         問4確認用\n",
    "#         B = np.array([1., 2., 3.]) \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期値クラス  XavierInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2, Fil_w=None):\n",
    "        self.sigma = 1/np.sqrt(n_nodes1)\n",
    "        if Fil_w == None:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        else:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2, Fil_w)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初期値クラス  HeInitializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    def W(self, n_nodes1, n_nodes2, Fil_w=None):\n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        if Fil_w == None:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        else:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2, Fil_w)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.w -= self.lr*layer.dW\n",
    "        layer.b -= self.lr*layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, hw, hb):\n",
    "        self.lr = lr\n",
    "        self.hw = hw\n",
    "        self.hb = hb\n",
    "        \n",
    "    def update(self, layer):\n",
    "        self.hw = layer.dW**2 + self.hw\n",
    "        layer.w -= self.lr * np.sqrt(self.hw) * layer.dW\n",
    "        \n",
    "        self.hb = layer.dB**2 + self.hb\n",
    "        layer.b -= self.lr * np.sqrt(self.hb) *  layer.dB\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.Z = np.maximum(A, 0)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self,dZ):\n",
    "        dA = np.where(self.Z > 0, dZ, 0)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数 Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.Z = np.exp(A)/np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, ty):\n",
    "        dA = self.Z - ty\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数 Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        self.Z = 1/(1+np.exp(-A)) \n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - self.Z) * self.Z\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数 Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - self.Z**2)    #tanh関数の微分\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FC層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.w = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.b = initializer.B(n_nodes2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.X = X\n",
    "        A = np.dot(self.X, self.w) + self.b\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dW = np.dot(self.X.T, dA)\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "        self.dZ = np.dot(dA, self.w.T)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込み層 SimpleConv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    In_ch : 入力チャネル数\n",
    "    Out_ch : フィルタ枚数=出力チャネル数\n",
    "    n_features : 入力特徴量数\n",
    "    Out_w : 出力サイズ\n",
    "    Fil_w : フィルタ幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    \n",
    "    Out_w : 出力サイズ（幅）\n",
    "    X : 次の形のndarray, shape(In_ch、n_features)\n",
    "    w : 次の形のndarray, shape(Out_ch、In_ch、Fil_w)\n",
    "    b : 次の形のndarray, shape（Out_ch）\n",
    "    \"\"\"\n",
    "    def __init__(self, In_ch, Out_ch, n_features, Out_w, Fil_w, stride, pad, initializer, optimizer):\n",
    "        self.In_ch = In_ch\n",
    "        self.Out_ch = Out_ch\n",
    "        self.Out_w = Out_w\n",
    "        self.n_features = n_features\n",
    "        self.Fil_w = Fil_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.w = initializer.W(self.Out_ch, self.In_ch, self.Fil_w)    # フィルタサイズ\n",
    "        self.b = initializer.B(self.Out_ch)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Xが1次元なら(1, n_features)にreshape\n",
    "        if X.ndim == 1:\n",
    "            self.X = X.reshape(-1,self.n_features)\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        A = np.zeros(self.Out_ch * self.Out_w)    #出力A用の入れ物の用意\n",
    "        A = A.reshape(self.Out_ch, self.Out_w)    #shape(出力チャネル,出力特徴量)\n",
    "\n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                A[i][k] = np.sum(np.sum(self.X[ :, k : (k + self.Fil_w)] * self.w[i], axis=1))+ self.b[i]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        self.dW = np.zeros((self.Out_ch, self.In_ch, self.Fil_w))\n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            s = np.zeros(self.In_ch * self.Fil_w)    # shape(入力チャネル数、フィルタサイズ)\n",
    "            s = s.reshape(self.In_ch, self.Fil_w)\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                s += dA[i][k] * self.X[ :, k : (k + self.Fil_w)]\n",
    "            self.dW[i] = s\n",
    "        \n",
    "        self.dX = np.zeros(self.In_ch * self.n_features)\n",
    "        self.dX = self.dX.reshape(self.In_ch, self.n_features)\n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                self.dX[:, 0+self.stride*k : self.Fil_w + self.stride*k] += dA[i][k] * self.w[i]\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d_q4:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    In_ch : 入力チャネル数\n",
    "    Out_ch : フィルタ枚数=出力チャネル数\n",
    "    n_features : 特徴量数\n",
    "    Fil_w : フィルタ幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "    \n",
    "    Out_w : 出力サイズ（幅）\n",
    "    X : 次の形のndarray, shape(In_ch、n_features)\n",
    "    w : 次の形のndarray, shape(Out_ch、In_ch、Fil_w)\n",
    "    b : 次の形のndarray, shape（Out_ch）\n",
    "    \"\"\"\n",
    "    def __init__(self, In_ch, Out_ch, n_features, Fil_w, stride, pad, initializer, optimizer):\n",
    "        self.In_ch = In_ch\n",
    "        self.Out_ch = Out_ch\n",
    "        self.n_features = n_features\n",
    "        self.Fil_w = Fil_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.w = initializer.W(self.Out_ch, self.In_ch, self.Fil_w)    # フィルタサイズ\n",
    "        self.b = initializer.B(self.Out_ch)\n",
    "        \n",
    "        # 出力サイズを計算\n",
    "        self.Out_w = self.calc_out_shape()\n",
    "    \n",
    "    def calc_out_shape(self):\n",
    "        # 出力サイズの計算\n",
    "        return int(1 + (self.n_features + 2*self.pad - self.Fil_w)/self.stride)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Xが1次元なら(1, n_features)にreshape\n",
    "        if X.ndim == 1:\n",
    "            self.X = X.reshape(-1,self.n_features)\n",
    "        else:\n",
    "            self.X = X\n",
    "        \n",
    "        A = np.zeros(self.Out_ch * self.Out_w)    #出力A用の入れ物の用意\n",
    "        A = A.reshape(self.Out_ch, self.Out_w)    #shape(出力チャネル,出力特徴量)\n",
    "        \n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                A[i][k] = np.sum(np.sum(self.X[ :, k : (k + self.Fil_w)] * self.w[i], axis=1))+ self.b[i]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        self.dW = np.zeros((self.Out_ch, self.In_ch, self.Fil_w))\n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            s = np.zeros(self.In_ch * self.Fil_w)    # shape(入力チャネル数、フィルタサイズ)\n",
    "            s = s.reshape(self.In_ch, self.Fil_w)\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                s += dA[i][k] * self.X[ :, k : (k + self.Fil_w)]\n",
    "            self.dW[i] = s\n",
    "        \n",
    "        self.dX = np.zeros(self.In_ch * self.n_features)\n",
    "        self.dX = self.dX.reshape(self.In_ch, self.n_features)\n",
    "        for i in range(self.Out_ch):    #フィルタ数＝出力チャネル数\n",
    "            for k in range(self.Out_w):    # 出力サイズ\n",
    "                self.dX[:, 0+self.stride*k : self.Fil_w + self.stride*k] += dA[i][k] * self.w[i]\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch1dCNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    def __init__(self, filter_list, lr=0.01 ,sigma=0.01, n_output=10):\n",
    "        self.lr = lr     # 学習係数\n",
    "        self.sigma = sigma    # 重みとバイアスの広がり具合\n",
    "        self.n_output = n_output\n",
    "        self.filter_list = filter_list    # フィルタ枚数のリスト\n",
    "    \n",
    "    def fit(self, X, y, In_ch, Initializers, activations, Fil_w=3, stride=1, pad=0, opt=\"SGD\", epoch=1):\n",
    "        self.n_features = X.shape[-1]\n",
    "        self.loss = np.zeros(epoch)\n",
    "        self.activation_list = []\n",
    "        self.FC_list = []\n",
    "        self.conv_list = []\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "        self.Fil_w = Fil_w\n",
    "        \n",
    "        # conv n_node1:In_ch, n_node2:Out_w / FC  n_node1:入力特徴量,n_node2:出力層サイズ\n",
    "        n_node1 = self.n_features\n",
    "        # conv→FC reshape用 flat_Out_ch=フィルタ枚数\n",
    "        self.flat_Out_ch,self.flat_Out_w = 0, 0\n",
    "        \n",
    "        # インスタンス生成---------------------------------------------------\n",
    "        for i in range(len(Initializers)):\n",
    "            # 初期値----------------------------------------\n",
    "            if Initializers[i] == \"Simple\":\n",
    "                initializ = SimpleInitializer(self.sigma)\n",
    "            elif Initializers[i] == \"Xavier\":\n",
    "                initializ = XavierInitializer()\n",
    "            else:\n",
    "                initializ = HeInitializer()\n",
    "            # 最適化手法-----------------------------------\n",
    "            if opt == \"SGD\":\n",
    "                optimizer = SGD(self.lr)\n",
    "            else:\n",
    "                optimizer = AdaGrad(self.lr, 0, 0)\n",
    "            # 畳み込み層インスタンス生成-------------------\n",
    "            if i < len(self.filter_list):\n",
    "                n_node2 = self.calc_out_shape(n_node1)    # 出力幅の計算\n",
    "                self.conv_list.append(SimpleConv1d(In_ch, self.filter_list[i], n_node1, n_node2, Fil_w, stride, pad, initializ, optimizer))\n",
    "            else:\n",
    "                self.FC_list.append(FC(n_node1, self.n_output, initializ, optimizer))\n",
    "                \n",
    "            if i == len(self.filter_list)-1:\n",
    "                self.flat_Out_ch, self.flat_Out_w = self.filter_list[-1], n_node2    # reshape用に値を取っておく\n",
    "                n_node1 = self.filter_list[-1]*n_node2    # conv→FCのreshape対応\n",
    "            else:\n",
    "                n_node1 = n_node2\n",
    "            \n",
    "            # 活性化関数インスタンス生成----------------\n",
    "            if i < len(self.filter_list):\n",
    "                if activations[i] == \"tanh\":\n",
    "                    self.activation_list.append(Tanh() )\n",
    "                elif activations[i] == \"sigmoid\":\n",
    "                    self.activation_list.append(Sigmoid() )\n",
    "                else:\n",
    "                    self.activation_list.append(ReLU() )\n",
    "            else:\n",
    "                self.activation_list.append(Softmax() )\n",
    "        # -----------------------------------------------------------------end\n",
    "           \n",
    "        for i in range(epoch):\n",
    "            \n",
    "            A = self.conv_list[0].forward(X)    # 畳み込み層 1層目\n",
    "            Z = self.activation_list[0].forward(A)\n",
    "            \n",
    "            for j in range(1, len(self.conv_list) -1):    # 畳み込み層 2層目以降\n",
    "                A = self.conv_list[j].forward(Z)\n",
    "                Z = self.activation_list[j].forward(A)\n",
    "            \n",
    "            # conv→FCのreshape バッチサイズ=1なので1\n",
    "            Z = Z.reshape(1, self.flat_Out_ch*self.flat_Out_w)\n",
    "            \n",
    "            A = self.FC_list[-1].forward(Z)    # 出力層\n",
    "            Z = self.activation_list[-1].forward(A)\n",
    "            \n",
    "            dA = self.activation_list[-1].backward(y) # 出力層\n",
    "            dZ = self.FC_list[-1].backward(dA)\n",
    "            # FC→convのreshape\n",
    "            dZ = dZ.reshape(self.flat_Out_ch, self.flat_Out_w)\n",
    "            \n",
    "            for j in reversed(range(1, len(self.conv_list))):    # 中間層\n",
    "                dA = self.activation_list[j].backward(dZ)\n",
    "                dZ = self.conv_list[j].backward(dA)\n",
    "\n",
    "            dA = self.activation_list[0].backward(dZ)    # 入力層\n",
    "            dZ = self.conv_list[0].backward(dA)\n",
    "                \n",
    "                \n",
    "            yp = self.predict(X)\n",
    "            # 損失関数\n",
    "            self.loss[i] = -np.sum(y * np.log(self.predict_y))/ self.predict_y.shape[1]\n",
    "            \n",
    "    def calc_out_shape(self, n_node1):\n",
    "        # 出力サイズの計算\n",
    "        return int(1 + (n_node1 + 2*self.pad - self.Fil_w)/self.stride)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        A = self.conv_list[0].forward(X)    # 入力層\n",
    "        Z = self.activation_list[0].forward(A)\n",
    "        \n",
    "        for j in range(1, len(self.conv_list)-1):    # 中間層&出力層\n",
    "            A = self.conv_list[j].forward(Z)\n",
    "            Z = self.activation_list[j].forward(A)\n",
    "            \n",
    "        # conv→FCのreshape バッチサイズ=1なので1\n",
    "        Z = Z.reshape(1, self.flat_Out_ch*self.flat_Out_w)\n",
    "        \n",
    "        A = self.FC_list[-1].forward(Z)\n",
    "        Z = self.activation_list[-1].forward(A)\n",
    "        self.predict_y = Z.copy()\n",
    "        \n",
    "        return np.argsort(Z)[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問4 テストデータ（ホワイトボード）での確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer_q4 = SimpleInitializer(0.01)\n",
    "optimizer_q4 = SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_ch, Out_ch, n_features, Fil_w, stride, pad =  2, 3, 4, 3, 1, 0\n",
    "conv_q4 = SimpleConv1d_q4(In_ch, Out_ch, n_features, Fil_w, stride, pad, initializer_q4, optimizer_q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21., 29.],\n",
       "       [18., 25.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_q4.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[125., 230., 204., 113.],\n",
       "       [102., 206., 195., 102.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_a =  np.array([[9, 11], [32, 35], [52, 56]])\n",
    "conv_q4.backward(delta_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問4 テストデータ（diver）での確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer_q4_d = SimpleInitializer(0.01)\n",
    "optimizer_q4_d = SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "In_ch, Out_ch, n_features, Fil_w, stride, pad =  2, 3, 4, 3, 1, 0\n",
    "conv_q4_d = SimpleConv1d(In_ch, Out_ch, n_features, Fil_w, stride, pad,\n",
    "                         initializer_q4_d, optimizer_q4_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16., 22.],\n",
       "       [17., 23.],\n",
       "       [18., 24.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_q4_d.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arisa/.pyenv/versions/anaconda3-5.3.1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=8)    # 小数点以下出力桁数\n",
    "np.set_printoptions(suppress=True)    #指数表示禁止\n",
    "# 《データセットをダウンロードするコード》\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "# 前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# one-hotエンコーディング\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_onehot = enc.fit_transform(y_train[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[0]\n",
    "y_train = y_onehot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_test = Scratch1dCNNClassifier(filter_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_test.fit(X_train, y_train, In_ch=1, Initializers=[\"Xavier\", \"Xavier\"], activations=[\"ReLU\", \"Softmax\"],\n",
    "                opt=\"AdaGrad\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21438927])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scCNN_test.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scCNN_test.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数チャンネルデータでも確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_test_1 = Scratch1dCNNClassifier(filter_list=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCNN_test_1.fit(x, y, In_ch=2, Initializers=[\"Xavier\", \"Xavier\"], activations=[\"ReLU\", \"Softmax\"],\n",
    "                opt=\"AdaGrad\", epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scCNN_test_1.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "224.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
